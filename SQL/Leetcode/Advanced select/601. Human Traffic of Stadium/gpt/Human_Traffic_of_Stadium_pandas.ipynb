{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas 2.2.2用\n",
    "\n",
    "## 0) 前提\n",
    "\n",
    "* 環境: **Python 3.10.15 / pandas 2.2.2**\n",
    "* **指定シグネチャ厳守**（この回答では `stadium_consecutive` を定義）\n",
    "* I/O 禁止、不要な `print` や `sort_values` 禁止（必要な並び替えは **NumPy の `argsort`** で実施）\n",
    "* 判定は **ID 連続**、出力は **`id, visit_date, people`**（`visit_date` 昇順）\n",
    "\n",
    "## 1) 問題\n",
    "\n",
    "* `3 行以上の「連続した id」を持ち、かつ各行 people >= 100 のレコードを抽出する。結果は visit_date 昇順。`\n",
    "* 入力 DF: `stadium(id: int, visit_date: datetime64[ns] or date-like, people: int)`\n",
    "* 出力: `['id','visit_date','people']` — 連続 ID の島（長さ ≥ 3）に属する行のみ\n",
    "\n",
    "## 2) 実装（指定シグネチャ厳守）\n",
    "\n",
    "> 列最小化 → `people >= 100` で縮小 → `id - row_number(order by id)` で島キー → グループサイズでフィルタ → `visit_date` 昇順に整形（`np.argsort`）。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stadium_consecutive(stadium: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.DataFrame: 列名と順序は ['id', 'visit_date', 'people']\n",
    "    \"\"\"\n",
    "    # 必要列のみ\n",
    "    df = stadium[['id', 'visit_date', 'people']]\n",
    "\n",
    "    # people >= 100 のみ対象（以降の計算対象を縮小）\n",
    "    df_hot = df[df['people'] >= 100]\n",
    "\n",
    "    if df_hot.empty:\n",
    "        # 空ならそのまま空の所定列を返す\n",
    "        return df_hot[['id', 'visit_date', 'people']]\n",
    "\n",
    "    # ---- row_number() over (order by id) を numpy で実装（sort_values を使わない） ----\n",
    "    ids = df_hot['id'].to_numpy()\n",
    "    order_idx = np.argsort(ids, kind='mergesort')  # 安定ソート\n",
    "    rn = np.empty_like(order_idx)                  # rn を id の順番に 1..n で割り振る\n",
    "    rn[order_idx] = np.arange(1, order_idx.size + 1)\n",
    "\n",
    "    # 連番島キー: id - row_number\n",
    "    grp_key = ids - rn\n",
    "\n",
    "    # 島の長さを算出（groupby-aggregate → transform(size) 相当）\n",
    "    # 高速化のため pandas の groupby を使う\n",
    "    island_len = (\n",
    "        pd.Series(grp_key, index=df_hot.index)\n",
    "        .groupby(grp_key)\n",
    "        .transform('size')\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    # 長さ >= 3 の島に属する行のみ\n",
    "    mask = island_len >= 3\n",
    "    out = df_hot.loc[mask, ['id', 'visit_date', 'people']]\n",
    "\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    # 最終並び: visit_date 昇順（sort_values を使わず numpy で整列）\n",
    "    order_v = np.argsort(out['visit_date'].to_numpy(), kind='mergesort')\n",
    "    out = out.iloc[order_v]\n",
    "\n",
    "    # インデックスは連番に整える（表示の安定性向上）\n",
    "    out = out.reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "Analyze Complexity\n",
    "Runtime 310 ms\n",
    "Beats 59.45%\n",
    "Memory 67.59 MB\n",
    "Beats 60.59%\n",
    "\n",
    "```\n",
    "\n",
    "## 3) アルゴリズム説明\n",
    "\n",
    "* 使用 API / 手法\n",
    "\n",
    "  * `np.argsort` で **row_number の代替**（`id` 昇順の序数を付与）\n",
    "  * `id - row_number` で **Gaps-and-Islands** の島キーを作成\n",
    "  * `groupby(...).transform('size')` で島の長さ（各行に島サイズを付与）\n",
    "  * しきい値（≥3）で抽出\n",
    "  * 仕上げの並び替えは `np.argsort`（`sort_values` 非使用）\n",
    "* **NULL / 重複 / 型**\n",
    "\n",
    "  * `visit_date` は date/datetime いずれでも `np.argsort` で昇順化可能（NaT があれば末尾寄りになりますが、本問想定ではユニークで欠損なし）\n",
    "  * `id` は連番で単調増加が前提（テーブル仕様より）。同値行がないので `row_number` は一意\n",
    "  * `people` の判定は整数比較（欠損がある場合は事前に `fillna(-inf)` 等で対処）\n",
    "\n",
    "## 4) 計算量（概算）\n",
    "\n",
    "* `np.argsort`（`id` と `visit_date` の 2 回）: **O(N log N)**\n",
    "* `groupby(...).transform('size')`: **O(N)**\n",
    "* 総計: **O(N log N)**、メモリは `O(N)` で補助配列（整数/小規模）を使用\n",
    "\n",
    "## 5) 図解（Mermaid 超保守版）\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "  A[入力 DataFrame stadium] --> B[people >= 100 で縮小]\n",
    "  B --> C[id の row_number を算出]\n",
    "  C --> D[id - row_number で島キー]\n",
    "  D --> E[島ごとのサイズ >= 3 を抽出]\n",
    "  E --> F[visit_date 昇順で整形]\n",
    "  F --> G[出力 id, visit_date, people]\n",
    "```\n",
    "\n",
    "**さらに速く・省メモリ**にできます。ボトルネックは `groupby(...).transform('size')` と Series/CTE 的な中間オブジェクトです。\n",
    "PostgreSQL 版と同じ発想（連番の島＝**gaps-and-islands**）を、**純 NumPy** で実装するとだいぶ軽くなります。\n",
    "\n",
    "下は **`sort_values` 非使用**・**I/O/print なし**・**シグネチャ厳守** の最適化版です。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def stadium_consecutive(stadium: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.DataFrame: 列名と順序は ['id', 'visit_date', 'people']\n",
    "    \"\"\"\n",
    "    # 必要列だけ抽出（ビュー寄りに揃える）\n",
    "    df = stadium[['id', 'visit_date', 'people']]\n",
    "\n",
    "    # people >= 100 の行だけに縮小（ブール配列でコピーを極力回避）\n",
    "    hot_mask = df['people'].to_numpy() >= 100\n",
    "    if not np.any(hot_mask):\n",
    "        return df.iloc[0:0][['id', 'visit_date', 'people']]\n",
    "\n",
    "    hot = df.loc[hot_mask, ['id', 'visit_date', 'people']]\n",
    "\n",
    "    # ---- ここからは NumPy ベースで連続 ID の島を検出 ----\n",
    "    ids = hot['id'].to_numpy()\n",
    "    # row_number 相当は不要。id をソートして連続 run を直接求める\n",
    "    order = np.argsort(ids, kind='quicksort')         # 安定性不要なので quicksort\n",
    "    ids_sorted = ids[order]\n",
    "\n",
    "    # 連続ブレークを検知: 先頭は必ずブレーク\n",
    "    # diff==1 が「連続」、それ以外がブレーク\n",
    "    n = ids_sorted.size\n",
    "    brk = np.empty(n, dtype=bool)\n",
    "    brk[0] = True\n",
    "    if n > 1:\n",
    "        brk[1:] = np.diff(ids_sorted) != 1\n",
    "\n",
    "    # ラベル付け（累積和で run ID を振る）: 0..G-1\n",
    "    run_id = np.cumsum(brk) - 1\n",
    "\n",
    "    # 各 run の長さ（bincount は速くて省メモリ）\n",
    "    run_len = np.bincount(run_id)\n",
    "    keep_sorted = run_len[run_id] >= 3\n",
    "\n",
    "    if not np.any(keep_sorted):\n",
    "        return hot.iloc[0:0][['id', 'visit_date', 'people']]\n",
    "\n",
    "    # ソート順から元の行インデックスへ戻す\n",
    "    kept_idx = order[keep_sorted]\n",
    "    out = hot.iloc[kept_idx, :]  # 列順は ['id','visit_date','people'] のまま\n",
    "\n",
    "    # 指定：visit_date 昇順（sort_values は使わず np.argsort）\n",
    "    vorder = np.argsort(out['visit_date'].to_numpy(), kind='quicksort')\n",
    "    out = out.iloc[vorder].reset_index(drop=True)\n",
    "\n",
    "    return out[['id', 'visit_date', 'people']]\n",
    "\n",
    "Analyze Complexity\n",
    "Runtime 306 ms\n",
    "Beats 65.60%\n",
    "Memory 67.32 MB\n",
    "Beats 81.78%\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 改善ポイント（なぜ速い？）\n",
    "\n",
    "* **`groupby.transform('size')` を排除**\n",
    "  → `np.diff` + `np.cumsum` + `np.bincount` の **O(N)** 構成に置換。中間 Series も削減。\n",
    "* **`ROW_NUMBER` 風の計算をやめ、run-length encoding で直接「連続」を検出**\n",
    "  → 追加の配列1〜2本で済み、**メモリ節約**（特に N が大きいと効く）。\n",
    "* **`mergesort`→`quicksort`**\n",
    "  → 安定性不要な箇所はクイックソートで **純粋に速い**。\n",
    "* **列最小化**（最初から `['id','visit_date','people']` のみ）\n",
    "  → キャッシュ効率と一時オブジェクト削減。\n",
    "\n",
    "---\n",
    "\n",
    "## さらに詰めるなら（任意）\n",
    "\n",
    "* `id` / `people` を `int32` にダウンサイジング可能なら、事前に `astype('int32')` で**約半分のメモリ**。\n",
    "  （ただし上限値と欠損の有無に注意）\n",
    "* `visit_date` が文字列なら **事前に `datetime64[ns]` 化**で `argsort` が高速化。\n",
    "* 入力 DF が巨大で別処理も走る環境なら、**列アクセス時に `to_numpy(copy=False)`** を積極利用しコピー回避。\n",
    "\n",
    "---\n",
    "\n",
    "### 期待効果（目安）\n",
    "\n",
    "* 同等のデータ分布で、`groupby.transform('size')` 版から **10〜30% 程度の短縮**、\n",
    "  メモリは **Series/GroupBy の一時領域分が削減**されるケースが多いです。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d461c9db",
   "metadata": {},
   "source": [
    "## 0) 前提\n",
    "\n",
    "* 環境: **Python 3.10.15 / pandas 2.2.2**\n",
    "* **指定シグネチャ厳守**: `def daily_active_users(activity: pd.DataFrame) -> pd.DataFrame`\n",
    "* I/O 禁止、不要な `print` や `sort_values` は使用しない\n",
    "* Activity は重複行を含む可能性があるので、**(activity_date, user_id) でユニーク化してからカウント**\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 問題\n",
    "\n",
    "* `{{PROBLEM_STATEMENT}}`\n",
    "  `Activity` データフレームから、**2019-07-27 を含む直近 30 日間**における、日別アクティブユーザー数を求める。\n",
    "  あるユーザーが「その日に 1 回以上アクティビティを行っていれば」その日はアクティブとみなす。\n",
    "  アクティビティ種別（`activity_type`）は `'open_session', 'end_session', 'scroll_down', 'send_message'` のいずれも有効とみなす。\n",
    "  アクティブユーザー数が 0 の日は結果に含めない。\n",
    "\n",
    "* `{{INPUT_DATAFRAMES}}`\n",
    "\n",
    "  ```text\n",
    "  activity: pd.DataFrame\n",
    "  columns:\n",
    "    - user_id       (int)\n",
    "    - session_id    (int)\n",
    "    - activity_date (date または datetime64[ns] 相当)\n",
    "    - activity_type (category/str: 'open_session', 'end_session', 'scroll_down', 'send_message')\n",
    "  ```\n",
    "\n",
    "* `{{OUTPUT_COLUMNS_AND_RULES}}`\n",
    "\n",
    "  ```text\n",
    "  出力: pd.DataFrame\n",
    "  columns:\n",
    "    - day           (date)  … activity_date\n",
    "    - active_users  (int)   … その日に 1 回以上アクティビティを行ったユニーク user_id 数\n",
    "\n",
    "  ルール:\n",
    "    - 対象期間: 2019-06-28 ～ 2019-07-27（両端含む）…「2019-07-27 を含む 30 日間」\n",
    "    - アクティブ判定は「その日の (user_id, activity_date) が 1 回以上存在」\n",
    "    - ユーザー数 0 の日は含めない（=そもそも行が存在しない）\n",
    "    - 行順は任意（sort は不要）\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 実装（指定シグネチャ厳守）\n",
    "\n",
    "> 列最小化 → 期間フィルタ → `(user_id, activity_date)` でユニーク化 → `groupby` で日別カウント。\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def daily_active_users(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.DataFrame: 列名と順序は ['day', 'active_users']\n",
    "    \"\"\"\n",
    "    # 期間フィルタ: 2019-07-27 を含む直近 30 日間 → 2019-06-28〜2019-07-27\n",
    "    start = pd.to_datetime(\"2019-06-28\")\n",
    "    end = pd.to_datetime(\"2019-07-27\")\n",
    "    mask = (activity[\"activity_date\"] >= start) & (activity[\"activity_date\"] <= end)\n",
    "\n",
    "    # 列最小化 + ユニーク化: 1 ユーザーが 1 日に複数行あっても 1 回とカウントする\n",
    "    uniq = (\n",
    "        activity.loc[mask, [\"user_id\", \"activity_date\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # 日別にユーザー数をカウント（ユーザー ID のユニーク数）\n",
    "    result = (\n",
    "        uniq\n",
    "        .groupby(\"activity_date\", as_index=False)[\"user_id\"]\n",
    "        .nunique()\n",
    "    )\n",
    "\n",
    "    # 列名を仕様どおりに整形\n",
    "    result = result.rename(\n",
    "        columns={\n",
    "            \"activity_date\": \"day\",\n",
    "            \"user_id\": \"active_users\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "Analyze Complexity\n",
    "Runtime 316 ms\n",
    "Beats 50.66%\n",
    "Memory 68.35 MB\n",
    "Beats 38.32%\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) アルゴリズム説明\n",
    "\n",
    "使用している主な pandas API:\n",
    "\n",
    "1. **期間フィルタ**:\n",
    "\n",
    "   ```python\n",
    "   mask = (activity[\"activity_date\"] >= start) & (activity[\"activity_date\"] <= end)\n",
    "   activity.loc[mask, ...]\n",
    "   ```\n",
    "\n",
    "   * `activity_date` が `datetime64[ns]` / `date` でも比較可能。\n",
    "   * 2019-06-28〜2019-07-27 に含まれる行だけを対象とする。\n",
    "\n",
    "2. **列最小化 & ユニーク化**:\n",
    "\n",
    "   ```python\n",
    "   activity.loc[mask, [\"user_id\", \"activity_date\"]].drop_duplicates()\n",
    "   ```\n",
    "\n",
    "   * 使うのはアクティブ判定のキーだけなので、列を `user_id, activity_date` に絞る。\n",
    "   * 同じユーザーが同じ日に複数アクティビティをしていても、\n",
    "     `(user_id, activity_date)` のペアが一意になるように `drop_duplicates()`。\n",
    "\n",
    "3. **日別アクティブユーザー数**:\n",
    "\n",
    "   ```python\n",
    "   uniq.groupby(\"activity_date\", as_index=False)[\"user_id\"].nunique()\n",
    "   ```\n",
    "\n",
    "   * `groupby(\"activity_date\")` で日ごとにまとめる。\n",
    "   * 各日について `user_id` のユニーク数（`nunique()`）を取り、アクティブユーザー数とする。\n",
    "   * アクティビティが 1 件もない日付は `uniq` 自体に現れないため、\n",
    "     自然と結果にも含まれない（=「0 の日は除外」要件を満たす）。\n",
    "\n",
    "4. **列名整形**:\n",
    "\n",
    "   ```python\n",
    "   result.rename(columns={\"activity_date\": \"day\", \"user_id\": \"active_users\"})\n",
    "   ```\n",
    "\n",
    "   * 問題の要求どおり出力列名を `day`, `active_users` に揃える。\n",
    "   * 並び順も `[day, active_users]` になるようにしている。\n",
    "\n",
    "### NULL / 重複 / 型\n",
    "\n",
    "* **重複**:\n",
    "  `(user_id, activity_date)` で `drop_duplicates()` しているため、\n",
    "  同じユーザーが同日に 10 回アクションしても **1 ユーザーとしてカウント**される。\n",
    "* **NULL**:\n",
    "\n",
    "  * `activity_date` や `user_id` が `NaN` の行は、条件に合致しても\n",
    "    ペアとして扱いづらいため、通常はデータ仕様上「存在しない前提」。\n",
    "    （もし存在すれば、`NaN` を含む行も 1 ユニットとして扱われるので、\n",
    "    必要に応じて `dropna(subset=[\"user_id\", \"activity_date\"])` を噛ませてもよい。）\n",
    "* **型**:\n",
    "\n",
    "  * `activity_date` が文字列の場合でも、`to_datetime` 済みなら問題なく比較できる。\n",
    "  * `user_id` は `int` 前提だが、`nunique()` は dtype に依存せず機能する。\n",
    "\n",
    "---\n",
    "\n",
    "## 4) 計算量（概算）\n",
    "\n",
    "Activity の行数を **N**、期間内に残る行数を **N₃₀** とすると:\n",
    "\n",
    "1. **期間フィルタ** `activity.loc[mask, ...]`\n",
    "\n",
    "   * 各行を 1 回見て条件を評価 → **O(N)**\n",
    "\n",
    "2. **`drop_duplicates()`** on `[\"user_id\", \"activity_date\"]`\n",
    "\n",
    "   * ハッシュベースの重複除去 → **O(N₃₀)** 〜 **O(N₃₀ log N₃₀)** 近辺\n",
    "\n",
    "3. **`groupby(\"activity_date\")[\"user_id\"].nunique()`**\n",
    "\n",
    "   * グループ数を D（日数 ≤ 30）とすると\n",
    "     **O(N₃₀)**（ハッシュ集約）程度\n",
    "\n",
    "総じて、支配的なのは **O(N₃₀)**～**O(N₃₀ log N₃₀)**。\n",
    "日数 D は最大 30 と小さいため、D によるオーバーヘッドは無視できるレベルです。\n",
    "\n",
    "---\n",
    "\n",
    "## 5) 図解（Mermaid 超保守版）\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "  A[activity DF] --> B[期間フィルタ 2019-06-28〜2019-07-27]\n",
    "  B --> C[列最小化 user_id, activity_date]\n",
    "  C --> D[\"(user_id, activity_date) でdrop_duplicates\"]\n",
    "  D --> E[\"groupby(activity_date) user_id.nunique()\"]\n",
    "  E --> F[列名を day, active_users に整形]\n",
    "  F --> G[\"出力 DF (day, active_users)\"]\n",
    "```\n",
    "\n",
    "この実装は\n",
    "\n",
    "* I/O なし\n",
    "* `print` / `sort_values` 不使用\n",
    "* 指定シグネチャ・出力列仕様を満たす\n",
    "\n",
    "ので、LeetCode などの Pandas 問題でもそのまま利用できます。\n",
    "\n",
    "結論から言うと、\n",
    "\n",
    "* **316ms / 上位 50% は十分合格ライン**\n",
    "* LeetCode の Pandas 環境だと、**劇的な改善はほぼ出ません**\n",
    "* とはいえ、**drop_duplicates を消す & 中間 DF を減らす**くらいのチューニング余地はあります\n",
    "\n",
    "という感じです。\n",
    "\n",
    "---\n",
    "\n",
    "## 1) ロジック的な改善ポイント\n",
    "\n",
    "今のコードはざっくりこんな流れでした：\n",
    "\n",
    "```python\n",
    "mask = (activity[\"activity_date\"] >= start) & (activity[\"activity_date\"] <= end)\n",
    "\n",
    "uniq = (\n",
    "    activity.loc[mask, [\"user_id\", \"activity_date\"]]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "result = (\n",
    "    uniq\n",
    "    .groupby(\"activity_date\", as_index=False)[\"user_id\"]\n",
    "    .nunique()\n",
    ")\n",
    "```\n",
    "\n",
    "ここで\n",
    "\n",
    "* `(user_id, activity_date)` を `drop_duplicates` した後、\n",
    "* さらに `groupby().nunique()` で `user_id` のユニーク数を数えている\n",
    "\n",
    "ので、**「重複除去」を 2 回やっている**イメージになっています。\n",
    "\n",
    "この問題は「日ごとの user_id のユニーク数」が欲しいだけなので、\n",
    "\n",
    "* **`drop_duplicates` をやめて**\n",
    "* そのまま `groupby().nunique()` だけにする\n",
    "\n",
    "方がシンプル＆速くなる可能性が高いです。\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 改訂版コード（中間処理を削る）\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def daily_active_users(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.DataFrame: 列名と順序は ['day', 'active_users']\n",
    "    \"\"\"\n",
    "    # 定数は一度だけ to_datetime（ここはほぼオーバーヘッドになりません）\n",
    "    start = pd.to_datetime(\"2019-06-28\")\n",
    "    end = pd.to_datetime(\"2019-07-27\")\n",
    "\n",
    "    # 期間フィルタ ＋ 列最小化\n",
    "    filtered = activity.loc[\n",
    "        (activity[\"activity_date\"] >= start) & (activity[\"activity_date\"] <= end),\n",
    "        [\"user_id\", \"activity_date\"],\n",
    "    ]\n",
    "\n",
    "    # 日別に user_id のユニーク数をカウント\n",
    "    result = (\n",
    "        filtered\n",
    "        .groupby(\"activity_date\", as_index=False)[\"user_id\"]\n",
    "        .nunique()\n",
    "        .rename(columns={\"activity_date\": \"day\", \"user_id\": \"active_users\"})\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "Analyze Complexity\n",
    "Runtime 341 ms\n",
    "Beats 24.18%\n",
    "Memory 68.30 MB\n",
    "Beats 49.34%\n",
    "\n",
    "```\n",
    "\n",
    "### 変更点まとめ\n",
    "\n",
    "1. **`drop_duplicates()` を削除**\n",
    "\n",
    "   * `groupby(\"activity_date\")[\"user_id\"].nunique()` が「日 × user_id のユニーク数」を出してくれるので、\n",
    "     事前の `(user_id, activity_date)` ユニーク化は不要です。\n",
    "2. **マスクを直接 `.loc` の中で使っても良い**\n",
    "\n",
    "   * メモリ的にはほぼ同じですが、コードが少しスリムになります。\n",
    "3. **中間 DF を 1 つだけに削減**\n",
    "\n",
    "   * `uniq` をなくして `filtered` → `result` の 2 段構成に。\n",
    "\n",
    "これで計算量もメモリ使用量も、**わずかにだけ軽く**なります。\n",
    "\n",
    "---\n",
    "\n",
    "## 3) メモリ面の改善余地\n",
    "\n",
    "LeetCode のメモリ 68MB / Beat 38% も、正直かなり十分ですが、\n",
    "\n",
    "「理論上できること」としては：\n",
    "\n",
    "1. **使わない列を早めに捨てる**\n",
    "\n",
    "   ```python\n",
    "   filtered = activity.loc[mask, [\"user_id\", \"activity_date\"]]\n",
    "   ```\n",
    "\n",
    "   → これはすでにやっています。`session_id` / `activity_type` を持ち回っていないので OK。\n",
    "\n",
    "2. **型のダウンサイジング**\n",
    "\n",
    "   * 実務なら、`user_id`, `session_id` を `int32` に落としたり、`activity_type` を `category` にしてメモリ削減が効きます。\n",
    "   * ただし LeetCode では\n",
    "\n",
    "     * 変換コスト > メリット\n",
    "     * 評価環境の測り方次第\n",
    "       となることが多く、あまりスコアに出ないことも多いです。\n",
    "\n",
    "---\n",
    "\n",
    "## 4) どこまで頑張るべきか\n",
    "\n",
    "* Pandas 版で **Runtime Beats ~50%** は、\n",
    "  「アルゴリズムも実装も特に悪くない」ラインです。\n",
    "* Leaderboard 上位は\n",
    "\n",
    "  * CPython のバージョン差\n",
    "  * ジャストタイミングで速いマシンに当たる\n",
    "  * 評価のランダム誤差\n",
    "    なども効いてくるので、**数十 ms の差はほぼノイズ**です。\n",
    "\n",
    "なので、\n",
    "\n",
    "* ロジックをシンプルにして二重の重複除去をやめる（上記修正版）\n",
    "* それ以上は「趣味の世界」\n",
    "\n",
    "くらいに考えておくのが現実的かな、と思います。\n",
    "\n",
    "「pandas は入出力のフレームだけ、ロジックは全部 NumPy」でゴリ押ししてみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "## 1) 方針（全部 NumPy）\n",
    "\n",
    "やりたいことはシンプルに言うと：\n",
    "\n",
    "1. `activity_date` を 30 日間でフィルタ\n",
    "2. その期間内の `(activity_date, user_id)` ペアをユニークにする\n",
    "   → 「同じユーザーが同じ日に何回活動しても 1 回」\n",
    "3. 日別にペア数（= ユニーク user 数）を数える\n",
    "\n",
    "これを **全部 NumPy の `unique` でやる**、という作戦です。\n",
    "\n",
    "---\n",
    "\n",
    "## 2) 実装例（NumPy ゴリ押し版）\n",
    "\n",
    "LeetCode の Pandas 版シグネチャを想定して、こう書けます：\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def daily_active_users(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    NumPy メイン実装:\n",
    "      - フィルタ, 集約はすべて NumPy で実施\n",
    "      - pandas は列の取り出し & 最終 DataFrame 化のみ\n",
    "    Returns:\n",
    "        pd.DataFrame: 列名と順序は ['day', 'active_users']\n",
    "    \"\"\"\n",
    "    # --- 1) 必要列を NumPy 配列として取得 ---\n",
    "    # 日付は日単位の datetime64[D] にしておくと扱いやすい\n",
    "    dates = activity[\"activity_date\"].to_numpy(dtype=\"datetime64[D]\")\n",
    "    users = activity[\"user_id\"].to_numpy()\n",
    "\n",
    "    # --- 2) 30 日間の期間フィルタ ---\n",
    "    start = np.datetime64(\"2019-06-28\", \"D\")\n",
    "    end = np.datetime64(\"2019-07-27\", \"D\")\n",
    "    mask = (dates >= start) & (dates <= end)\n",
    "\n",
    "    dates = dates[mask]\n",
    "    users = users[mask]\n",
    "\n",
    "    if dates.size == 0:\n",
    "        # 対象期間に何もなければ空 DataFrame を返す\n",
    "        return pd.DataFrame({\"day\": pd.Series([], dtype=\"datetime64[ns]\"),\n",
    "                             \"active_users\": pd.Series([], dtype=\"int64\")})\n",
    "\n",
    "    # --- 3) (date, user) ペアを NumPy structured array で表現 ---\n",
    "    # これで np.unique でペア単位のユニークが取れる\n",
    "    pairs = np.empty(dates.shape[0], dtype=[(\"day\", \"datetime64[D]\"), (\"user\", users.dtype)])\n",
    "    pairs[\"day\"] = dates\n",
    "    pairs[\"user\"] = users\n",
    "\n",
    "    # --- 4) ペアをユニーク化（同じユーザーが同日に複数回いても 1 回に） ---\n",
    "    uniq_pairs = np.unique(pairs)  # ソートもかかる\n",
    "\n",
    "    # --- 5) 日付ごとにユニークユーザー数を数える ---\n",
    "    unique_days, counts = np.unique(uniq_pairs[\"day\"], return_counts=True)\n",
    "\n",
    "    # --- 6) pandas DataFrame に戻す ---\n",
    "    # np.datetime64[D] → DataFrame 生成時に datetime64[ns] に昇格する\n",
    "    result = pd.DataFrame({\n",
    "        \"day\": unique_days.astype(\"datetime64[ns]\"),\n",
    "        \"active_users\": counts.astype(\"int64\"),\n",
    "    })\n",
    "\n",
    "    return result\n",
    "\n",
    "Analyze Complexity\n",
    "Runtime 290 ms\n",
    "Beats 89.97%\n",
    "Memory 67.06 MB\n",
    "Beats 99.84%\n",
    "\n",
    "```\n",
    "\n",
    "ポイント:\n",
    "\n",
    "* **グループ化や重複除去は全部 `np.unique`** でやっているので、pandas の `groupby` / `drop_duplicates` は一切使っていません。\n",
    "* `structured array`（構造化配列）で `(day, user)` のタプルを 1 要素として扱うことで、\n",
    "  `np.unique` が「行単位でのユニーク」を取ってくれます。\n",
    "* `np.unique(uniq_pairs[\"day\"], return_counts=True)` で\n",
    "  「日付ごとの (日, ユーザー) ペア数」をまとめて計算。\n",
    "\n",
    "---\n",
    "\n",
    "## 3) アルゴリズムの流れ（NumPy 目線）\n",
    "\n",
    "1. **カラム抽出（pandas → NumPy）**\n",
    "\n",
    "   ```python\n",
    "   dates = activity[\"activity_date\"].to_numpy(\"datetime64[D]\")\n",
    "   users = activity[\"user_id\"].to_numpy()\n",
    "   ```\n",
    "\n",
    "2. **期間フィルタ（ブールマスク）**\n",
    "\n",
    "   ```python\n",
    "   mask = (dates >= start) & (dates <= end)\n",
    "   dates = dates[mask]\n",
    "   users = users[mask]\n",
    "   ```\n",
    "\n",
    "3. **構造化配列で `(day, user)` を表現**\n",
    "\n",
    "   ```python\n",
    "   pairs = np.empty(dates.shape[0], dtype=[(\"day\", \"datetime64[D]\"), (\"user\", users.dtype)])\n",
    "   pairs[\"day\"] = dates\n",
    "   pairs[\"user\"] = users\n",
    "   ```\n",
    "\n",
    "4. **ペアのユニーク化**\n",
    "\n",
    "   ```python\n",
    "   uniq_pairs = np.unique(pairs)\n",
    "   ```\n",
    "\n",
    "   これで「同じ user が同じ day に複数回活動していても一つにまとまる」。\n",
    "\n",
    "5. **日別にカウント**\n",
    "\n",
    "   ```python\n",
    "   unique_days, counts = np.unique(uniq_pairs[\"day\"], return_counts=True)\n",
    "   ```\n",
    "\n",
    "   `counts[i]` が `unique_days[i]` のユニーク user 数（= アクティブユーザー数）。\n",
    "\n",
    "6. **DataFrame へ逆変換**\n",
    "\n",
    "   ```python\n",
    "   pd.DataFrame({\"day\": unique_days.astype(\"datetime64[ns]\"),\n",
    "                 \"active_users\": counts.astype(\"int64\")})\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) 計算量 & パフォーマンスの雰囲気\n",
    "\n",
    "`N` = 30 日間に入る行数とすると、\n",
    "\n",
    "* 期間フィルタ: `O(N)`\n",
    "* 構造化配列作成: `O(N)`\n",
    "* `np.unique(pairs)`（ペア単位ユニーク + ソート）:\n",
    "  `O(N log N)`（内部ソート）くらい\n",
    "* `np.unique(uniq_pairs[\"day\"], return_counts=True)`:\n",
    "  ユニークな日数を `D` とすると `O(D log D)`（ただし D ≤ 30 なので誤差）\n",
    "\n",
    "⇒ **スループットの支配は 2 つの `np.unique`** で、\n",
    "概ね **O(N log N)** です。\n",
    "pandas の `groupby().nunique()` と同じオーダーですが、\n",
    "NumPy 直叩きのぶん、オーバーヘッドはだいぶ小さいです。\n",
    "\n",
    "---\n",
    "\n",
    "## 5) おまけ：さらに削るとしたら？\n",
    "\n",
    "* `activity_date` が最初から `datetime64[ns]` なら、`to_numpy(\"datetime64[D]\")` で日単位に丸める処理はそのまま速いです。\n",
    "* ユーザー ID を `int32` に落とせる（値の範囲が小さい）のであれば、\n",
    "  `users.astype(\"int32\")` でメモリを少し削ることもできます。\n",
    "  （LeetCode だとスコアに出ないことも多いですが。）\n",
    "\n",
    "---\n",
    "\n",
    "こんな感じで、ロジックはほぼ NumPy だけで完結させられます。\n",
    "「pandas 禁止縛りのコードレビュー」みたいな場でも、そのまま説明材料に使えるはずです。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
